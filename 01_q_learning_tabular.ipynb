{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "WARNING: Skipping gymnasium as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: package_name in c:\\users\\student\\appdata\\roaming\\python\\python38\\site-packages (0.1)\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install package_name --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gymnasium\n",
      "  Using cached gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\student\\appdata\\roaming\\python\\python38\\site-packages (from gymnasium) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Collecting numpy>=1.21.0\n",
      "  Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (1.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in c:\\users\\student\\appdata\\roaming\\python\\python38\\site-packages (from gymnasium) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\student\\appdata\\roaming\\python\\python38\\site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gymnasium) (3.20.2)\n",
      "Installing collected packages: numpy, gymnasium\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy-1.19.2.dist-info\\\\direct_url.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\nPlease upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\nUsers of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\nSee the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_q_table'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmy_q_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m q_table, discretize_state  \u001b[38;5;66;03m# or copy directly here\u001b[39;00m\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMountainCar-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'my_q_table'"
     ]
    }
   ],
   "source": [
    "import  gym\n",
    "import numpy as np\n",
    "from my_q_table import q_table, discretize_state  # or copy directly here\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    s_disc = discretize_state(state)\n",
    "    action = np.argmax(q_table[s_disc])\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Student\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "Episode     0 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode     1 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode   500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  1000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  1500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  2000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  2500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  3000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  3500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  4000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  4500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  5000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  5500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  6000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  6500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  7000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  7500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  8000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  8500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  9000 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "Episode  9500 | Reward: 0.00 | Avg(100): 0.000 | Epsilon: 0.300\n",
      "\n",
      "Learned Q-table after training:\n",
      "\n",
      "[[0.601 0.575 0.58  0.586]\n",
      " [0.486 0.151 0.199 0.56 ]\n",
      " [0.378 0.434 0.4   0.516]\n",
      " [0.403 0.116 0.174 0.449]\n",
      " [0.611 0.5   0.485 0.431]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.084 0.228 0.284 0.026]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.285 0.411 0.532 0.675]\n",
      " [0.553 0.754 0.388 0.499]\n",
      " [0.786 0.295 0.091 0.333]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.363 0.468 0.802 0.685]\n",
      " [0.87  0.859 0.868 0.844]\n",
      " [0.    0.    0.    0.   ]]\n",
      "\n",
      " Average reward over 100 evaluation episodes: 0.00\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import  gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- Initialize Environment ---\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)  # deterministic environment\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPSILON = 0.3      # exploration rate\n",
    "ALPHA = 0.3        # learning rate\n",
    "GAMMA = 0.99       # discount factor\n",
    "EPISODES = 10000\n",
    "\n",
    "# --- Q-table Initialization ---\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# --- Choose Action ---\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return env.action_space.sample()  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state, :])     # Exploit best known action\n",
    "\n",
    "# --- Update Q-table ---\n",
    "def update_q_table(state, action, reward, next_state, done):\n",
    "    best_next = np.max(Q[next_state, :])\n",
    "    Q[state, action] += ALPHA * (reward + GAMMA * best_next * (not done) - Q[state, action])\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_agent():\n",
    "    rewards_per_episode = []\n",
    "    for episode in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            update_q_table(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        # Progress print\n",
    "        if episode % 500 == 0 or episode == 1:\n",
    "            avg_reward = np.mean(rewards_per_episode[-100:])\n",
    "            print(f\"Episode {episode:5d} | Reward: {total_reward:.2f} | \"\n",
    "                  f\"Avg(100): {avg_reward:.3f} | Epsilon: {EPSILON:.3f}\")\n",
    "\n",
    "    return Q, rewards_per_episode\n",
    "        \n",
    "\n",
    "# --- Evaluation Function (place AFTER training) ---\n",
    "def evaluate_agent(episodes=100):\n",
    "    total_rewards = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(Q[state, :])  # Always take best known action\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "    avg_reward = total_rewards / episodes\n",
    "    print(f\"\\n Average reward over {episodes} evaluation episodes: {avg_reward:.2f}\")\n",
    "    return avg_reward\n",
    "\n",
    "# --- Run Training ---\n",
    "Q, reward_per_episode = train_agent()\n",
    "\n",
    "# --- Show Learned Q-table ---\n",
    "print(\"\\nLearned Q-table after training:\\n\")\n",
    "print(np.round(Q, 3))  # rounded for readability\n",
    "\n",
    "# --- Evaluate the Agent ---\n",
    "evaluate_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Student\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Create Environment ---\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# --- 2. Discretize the state space ---\n",
    "num_bins = (20, 20)  # (position, velocity)\n",
    "obs_space_low = env.observation_space.low\n",
    "obs_space_high = env.observation_space.high\n",
    "obs_space_bins = [\n",
    "    np.linspace(obs_space_low[i], obs_space_high[i], num_bins[i]) for i in range(len(num_bins))\n",
    "]\n",
    "\n",
    "def discretize_state(state):\n",
    "    \"\"\"Convert continuous state to discrete bin index tuple\"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(state)):\n",
    "        idx = np.digitize(state[i], obs_space_bins[i]) - 1\n",
    "        idx = min(num_bins[i]-1, max(0, idx))  # clamp to valid range\n",
    "        indices.append(idx)\n",
    "    return tuple(indices)\n",
    "\n",
    "# --- 3. Initialize Q-table ---\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(num_bins[0], num_bins[1], env.action_space.n))\n",
    "\n",
    "# --- 4. Hyperparameters ---\n",
    "alpha = 0.1        # Learning rate\n",
    "gamma = 0.99       # Discount factor\n",
    "epsilon = 1.0      # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "episodes = 5000\n",
    "\n",
    "# --- 5. Training Loop ---\n",
    "rewards_per_episode = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = discretize_state(env.reset()[0])\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Îµ-greedy action\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        next_state = discretize_state(next_obs)\n",
    "\n",
    "        # Update Q-value\n",
    "        best_next_q = np.max(q_table[next_state])\n",
    "        current_q = q_table[state + (action,)]\n",
    "        new_q = (1 - alpha) * current_q + alpha * (reward + gamma * best_next_q)\n",
    "        q_table[state + (action,)] = new_q\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if (episode+1) % 500 == 0:\n",
    "        avg_reward = np.mean(rewards_per_episode[-500:])\n",
    "        print(f\"Episode {episode+1}, avg reward (last 500): {avg_reward:.2f}\")\n",
    "\n",
    "# --- 6. Plot learning curve ---\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning on MountainCar-v0 with Bins')\n",
    "plt.show()\n",
    "\n",
    "# --- 7. Test the trained agent ---\n",
    "test_episodes = 5\n",
    "for ep in range(test_episodes):\n",
    "    obs = env.reset()[0]\n",
    "    state = discretize_state(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = np.argmax(q_table[state])\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        state = discretize_state(obs)\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Test Episode {ep+1}: Total Reward = {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: array([0.1, 0. ]), 1: array([0.86482753, 0.        ])}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = 0  # Initial state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            reward = 1\n",
    "            self.state = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            self.state = 0\n",
    "        return self.state, reward\n",
    "\n",
    "class SARSA:\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.actions))\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            return self.actions[np.argmax(self.q_table[state])]\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.actions))\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(len(self.actions))\n",
    "        \n",
    "        target = reward + self.gamma * self.q_table[next_state][self.actions.index(next_action)]\n",
    "        self.q_table[state][self.actions.index(action)] += self.alpha * (target - self.q_table[state][self.actions.index(action)])\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = Environment()\n",
    "actions = [0, 1]\n",
    "sarsa_agent = SARSA(actions)\n",
    "\n",
    "# Simulate an episode\n",
    "state = env.state\n",
    "action = sarsa_agent.get_action(state)\n",
    "for step in range(10):  # Run for 10 steps\n",
    "    next_state, reward = env.step(action)\n",
    "    next_action = sarsa_agent.get_action(next_state)\n",
    "    sarsa_agent.update(state, action, reward, next_state, next_action)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "env = Environment()\n",
    "actions = [0, 1]\n",
    "sarsa_agent = SARSA(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1:\n  State 0: [0.1 0. ]\n  State 1: [0. 0.]\nStep 2:\n  State 0: [0.1 0. ]\n  State 1: [0.1 0. ]\nStep 3:\n  State 0: [0.1 0. ]\n  State 1: [ 0.1   -0.091]\nStep 4:\n  State 0: [0.199 0.   ]\n  State 1: [ 0.1   -0.091]\nStep 5:\n  State 0: [0.199 0.   ]\n  State 1: [ 0.199 -0.091]\nStep 6:\n  State 0: [0.199 0.   ]\n  State 1: [ 0.27091 -0.091  ]\nStep 7:\n  State 0: [0.199 0.   ]\n  State 1: [ 0.27091 -0.16399]\nStep 8:\n  State 0: [0.3034819 0.       ]\n  State 1: [ 0.27091 -0.16399]\nStep 9:\n  State 0: [0.3034819 0.       ]\n  State 1: [ 0.3682009 -0.16399  ]\nStep 10:\n  State 0: [0.3034819 0.       ]\n  State 1: [ 0.46451889 -0.16399   ]\n\nBelow is the Q_Table:\n\nStep | Q(0,0) Q(0,1) | Q(1,0) Q(1,1)\n--------------------------------------\n   1 |  0.100  0.000 |  0.000  0.000\n   2 |  0.100  0.000 |  0.100  0.000\n   3 |  0.100  0.000 |  0.100 -0.091\n   4 |  0.199  0.000 |  0.100 -0.091\n   5 |  0.199  0.000 |  0.199 -0.091\n   6 |  0.199  0.000 |  0.271 -0.091\n   7 |  0.199  0.000 |  0.271 -0.164\n   8 |  0.303  0.000 |  0.271 -0.164\n   9 |  0.303  0.000 |  0.368 -0.164\n  10 |  0.303  0.000 |  0.465 -0.164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            reward = 1\n",
    "            self.state = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            self.state = 0\n",
    "        return self.state, reward\n",
    "\n",
    "class SARSA:\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.actions))\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            return self.actions[np.argmax(self.q_table[state])]\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.actions))\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(len(self.actions))\n",
    "        \n",
    "        target = reward + self.gamma * self.q_table[next_state][self.actions.index(next_action)]\n",
    "        self.q_table[state][self.actions.index(action)] += self.alpha * (target - self.q_table[state][self.actions.index(action)])\n",
    "\n",
    "env = Environment()\n",
    "actions = [0, 1]\n",
    "sarsa_agent = SARSA(actions)\n",
    "\n",
    "q_history = []\n",
    "\n",
    "state = env.state\n",
    "action = sarsa_agent.get_action(state)\n",
    "for step in range(10):\n",
    "    next_state, reward = env.step(action)\n",
    "    next_action = sarsa_agent.get_action(next_state)\n",
    "    sarsa_agent.update(state, action, reward, next_state, next_action)\n",
    "    \n",
    "    q_snapshot = {}\n",
    "    for s in [0, 1]:\n",
    "        if s in sarsa_agent.q_table:\n",
    "            q_snapshot[s] = sarsa_agent.q_table[s].copy()\n",
    "        else:\n",
    "            q_snapshot[s] = np.zeros(len(actions))\n",
    "    q_history.append(q_snapshot)\n",
    "    \n",
    "    state, action = next_state, next_action\n",
    "\n",
    "\n",
    "    print(f\"Step {step + 1}:\")\n",
    "    for s, q_values in sarsa_agent.q_table.items():\n",
    "        print(f\"  State {s}: {q_values}\")\n",
    "    \n",
    "    state, action = next_state, next_action\n",
    "print(\"\")\n",
    "\n",
    "print(\"Below is the Q_Table:\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f\"{'Step':>4} | {'Q(0,0)':>6} {'Q(0,1)':>6} | {'Q(1,0)':>6} {'Q(1,1)':>6}\")\n",
    "print(\"-\" * 38)\n",
    "for i, q in enumerate(q_history, 1):\n",
    "    print(f\"{i:>4} | {q[0][0]:6.3f} {q[0][1]:6.3f} | {q[1][0]:6.3f} {q[1][1]:6.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}